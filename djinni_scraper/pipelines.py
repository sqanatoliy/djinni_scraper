# Define your item pipelines here
#
# Don't forget to add your pipeline to the ITEM_PIPELINES setting
# See: https://docs.scrapy.org/en/latest/topics/item-pipeline.html


# useful for handling different item types with a single interface
import sqlite3
import json
from itemadapter import ItemAdapter

from djinni_scraper.utils.telegram_utils import Telegram


class DjinniScraperPipeline:
    def process_item(self, item, spider):
        return item


class SQLitePipeline:
    DB_PATH = "jobs.db"

    def __init__(self):
        self.conn = None
        self.cursor = None

    def open_spider(self, spider):
        """ –ü—ñ–¥–∫–ª—é—á–µ–Ω–Ω—è –¥–æ –±–∞–∑–∏ –ø—Ä–∏ —Å—Ç–∞—Ä—Ç—ñ —Å–ø–∞–π–¥–µ—Ä–∞ """
        try:
            spider.logger.info("üì° Opening SQLite pipeline.")
            self.conn = sqlite3.connect(self.DB_PATH)
            self.cursor = self.conn.cursor()

            self.cursor.execute("""
                CREATE TABLE IF NOT EXISTS vacancies (
                    id INTEGER PRIMARY KEY AUTOINCREMENT,
                    title TEXT,
                    salary TEXT,
                    company TEXT,
                    views INTEGER,
                    responses INTEGER,
                    pub_date TEXT NOT NULL,
                    truncate_description TEXT,
                    tags TEXT,
                    category TEXT,
                    url TEXT NOT NULL,
                    created_at TIMESTAMP DEFAULT CURRENT_TIMESTAMP,
                    UNIQUE(url, pub_date)
                )
            """)

            # –î–æ–¥–∞—î–º–æ —ñ–Ω–¥–µ–∫—Å –¥–ª—è —à–≤–∏–¥–∫–æ–≥–æ –ø–æ—à—É–∫—É –≤–∞–∫–∞–Ω—Å—ñ–π
            self.cursor.execute("""
                CREATE INDEX IF NOT EXISTS idx_url_pubdate ON vacancies (url, pub_date);
            """)

            self.conn.commit()
            spider.logger.info("‚úÖ Table checked or created successfully.")

        except sqlite3.Error as e:
            spider.logger.error(f"‚ùå Error connecting to SQLite: {e}")
            raise

    def close_spider(self, spider):
        """ –ó–∞–∫—Ä–∏–≤–∞—î–º–æ –∑'—î–¥–Ω–∞–Ω–Ω—è –∑ –±–∞–∑–æ—é –ø—Ä–∏ –∑–∞–≤–µ—Ä—à–µ–Ω–Ω—ñ —Ä–æ–±–æ—Ç–∏ """
        try:
            spider.logger.info("Closing SQLite pipeline.")
            if self.cursor:
                self.cursor.close()
            if self.conn:
                self.conn.close()
                spider.logger.info("‚úÖ Connection successfully closed.")
        except sqlite3.Error as e:
            spider.logger.error(f"‚ùå Error closing SQLite connection: {e}")

    def process_item(self, item, spider):
        """ –ü–µ—Ä–µ–≤—ñ—Ä—è—î —Ç–∞ –¥–æ–¥–∞—î –≤–∞–∫–∞–Ω—Å—ñ—é –≤ –±–∞–∑—É """
        telegram = Telegram(spider)
        try:
            adapter = ItemAdapter(item)
            url = adapter.get('url')
            pub_date = adapter.get('pub_date')

            if not url or not pub_date:
                spider.logger.warning("‚ö†Ô∏è Item does not have a valid URL or pub_date. Skipping insert.")
                return item

            # –ü–µ—Ä–µ–≤—ñ—Ä—è—î–º–æ, —á–∏ —ñ—Å–Ω—É—î –≤–∞–∫–∞–Ω—Å—ñ—è –∑ —Ç–∞–∫–∏–º `url` —ñ `pub_date`
            self.cursor.execute("""
                SELECT 1 FROM vacancies WHERE url = ? AND pub_date = ?
            """, (url, pub_date))

            if self.cursor.fetchone():
                spider.logger.info(f"üîÑ Vacancy already exists: {url} - {pub_date}. Skipping insert.")
                return item

            tags = json.dumps(adapter.get('tags', []))

            # –î–æ–¥–∞—î–º–æ –≤–∞–∫–∞–Ω—Å—ñ—é
            data = {
                "title": adapter.get('title', 'No Title'),
                "salary": adapter.get('salary', 'Not specified'),
                "company": adapter.get('company', 'Unknown'),
                "views": adapter.get('views', 0),
                "responses": adapter.get('responses', 0),
                "pub_date": pub_date,
                "truncate_description": adapter.get('truncate_description', ''),
                "tags": tags,
                "url": url,
                "category": adapter.get('category'),
            }

            self.cursor.execute("""
                INSERT INTO vacancies (title, salary, company, views, responses, pub_date, 
                                       truncate_description, tags, category, url) 
                VALUES (:title, :salary, :company, :views, :responses, :pub_date, 
                        :truncate_description, :tags, :category, :url)
            """, data)
            telegram.send_job_to_telegram(data)
            self.conn.commit()
            spider.logger.info(f"‚úÖ Vacancy saved: {url} - {pub_date}")

        except sqlite3.Error as e:
            spider.logger.error(f"‚ùå Database error while saving item: {e}")
            self.conn.rollback()

        except Exception as e:
            spider.logger.error(f"‚ùå Unexpected error in process_item: {e}")

        return item
