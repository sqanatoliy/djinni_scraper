import os
from urllib.parse import urljoin as urllib_urljoin

import scrapy
from decouple import config
from playwright.async_api import Page
from playwright.async_api import TimeoutError as PlaywrightTimeoutError

from djinni_scraper.utils.url_utils import get_start_url
from ..items import DjinniScraperItem

DJINNI_EMAIL = config("DJINNI_EMAIL")
DJINNI_PASSWORD = config("DJINNI_PASSWORD")


class DjinniSelectors:
    LOGIN_SELECTOR = "ul > li:nth-child(1) > a.jobs-push-login-link"
    EMAIL_SELECTOR = "#email"
    PASSWORD_SELECTOR = "#password"
    LOGIN_BUTTON_SELECTOR = 'button.btn-primary[type="submit"]'
    PROFILE_SELECTOR = "div.navbar-collapse div.user-name"
    JOBS_LIST_SELECTOR = "ul.list-jobs > li"
    PAGINATION_ITEMS_SELECTOR = "ul.pagination_with_numbers li.page-item"


class DjinniSpider(scrapy.Spider):
    name = "djinni"
    allowed_domains = ["djinni.co"]

    def __init__(self, **kwargs):
        super().__init__(**kwargs)
        self.params = {}
        for key, value in kwargs.items():
            if "," in value:
                self.params[key] = value.split(",")
            else:
                self.params[key] = value

        self.start_urls = [get_start_url(self.params)]
        self.category = self.params.get("primary_keyword", "all")

    def start_requests(self):
        """–ü–µ—Ä–µ–≤—ñ—Ä—è—î–º–æ, —á–∏ —î –∑–±–µ—Ä–µ–∂–µ–Ω–∏–π —Å—Ç–∞–Ω. –Ø–∫—â–æ —î ‚Äî –≤–∏–∫–æ—Ä–∏—Å—Ç–æ–≤—É—î–º–æ, —ñ–Ω–∞–∫—à–µ –ª–æ–≥—ñ–Ω–∏–º–æ—Å—å."""
        if os.path.exists("state.json"):
            self.logger.info(f"‚úÖ –ó–Ω–∞–π–¥–µ–Ω–æ state.json! –í–∏–∫–æ—Ä–∏—Å—Ç–æ–≤—É—î–º–æ –∑–±–µ—Ä–µ–∂–µ–Ω—É —Å–µ—Å—ñ—é. URL: {self.start_urls[0]}")
            yield scrapy.Request(
                url=self.start_urls[0],
                callback=self.parse_jobs,
                meta={
                    "playwright": True,
                    "playwright_context": "default",
                    "playwright_include_page": True,
                },
                errback=self.errback_close_page,
            )
        else:
            self.logger.info("üîÑ –í–∏–∫–æ–Ω—É—î–º–æ –∞–≤—Ç–æ—Ä–∏–∑–∞—Ü—ñ—é...")
            yield scrapy.Request(
                url="https://djinni.co/",
                callback=self.login,
                meta={
                    "playwright": True,
                    "playwright_context": "default",
                    "playwright_include_page": True,
                },
                errback=self.errback_close_page,
            )

    @staticmethod
    async def log_request(route, request):
        print(f"üì° –ó–∞–ø–∏—Ç: {request.url} | –ú–µ—Ç–æ–¥: {request.method}")
        await route.continue_()

    async def login(self, response):
        """Autologin to Djinni.co."""
        page = response.meta["playwright_page"]

        await page.route("**", self.log_request)
        await page.wait_for_selector(DjinniSelectors.LOGIN_SELECTOR)
        await page.click(DjinniSelectors.LOGIN_SELECTOR)

        await page.wait_for_selector(DjinniSelectors.LOGIN_BUTTON_SELECTOR, timeout=10000)
        await page.wait_for_timeout(3000)
        await page.fill(DjinniSelectors.EMAIL_SELECTOR, DJINNI_EMAIL)
        await page.fill(DjinniSelectors.PASSWORD_SELECTOR, DJINNI_PASSWORD)
        await page.click(DjinniSelectors.LOGIN_BUTTON_SELECTOR)

        try:
            await page.wait_for_selector(DjinniSelectors.PROFILE_SELECTOR, timeout=10000)
            self.logger.info("‚úÖ –õ–æ–≥—ñ–Ω —É—Å–ø—ñ—à–Ω–∏–π! –ó–±–µ—Ä—ñ–≥–∞—î–º–æ state.json.")
            await page.context.storage_state(path="state.json")
        except PlaywrightTimeoutError as err:
            self.logger.error(f"‚ùå –ü–æ–º–∏–ª–∫–∞ –ª–æ–≥—ñ–Ω—É! {err} –í–∏–¥–∞–ª—è—î–º–æ state.json.")
            if os.path.exists("state.json"):
                os.remove("state.json")
        finally:
            await page.close()
        yield scrapy.Request(
            url=self.start_urls[0],
            callback=self.parse_jobs,
            meta={
                "playwright": True,
                "playwright_context": "default",
                "playwright_include_page": True,
            },
            errback=self.errback_close_page,
        )

    async def parse_jobs(self, response):
        """–ü–∞—Ä—Å–∏–º–æ –≤–∞–∫–∞–Ω—Å—ñ—ó –ø—ñ—Å–ª—è –∞–≤—Ç–æ—Ä–∏–∑–∞—Ü—ñ—ó."""
        page = response.meta["playwright_page"]
        if not page:
            self.logger.error("‚ùå –ù–µ –æ—Ç—Ä–∏–º–∞–Ω–æ Playwright page. –ó–∞–≤–∞–Ω—Ç–∞–∂—É—î–º–æ –±–µ–∑ –Ω—å–æ–≥–æ.")
            return
        try:
            await page.wait_for_selector(DjinniSelectors.PROFILE_SELECTOR, timeout=5000)
            self.logger.info("‚úÖ –ê–≤—Ç–æ—Ä–∏–∑–∞—Ü—ñ—è –∞–∫—Ç–∏–≤–Ω–∞.")
        except PlaywrightTimeoutError:
            self.logger.warning("‚ùå –ê–≤—Ç–æ—Ä–∏–∑–∞—Ü—ñ—è –Ω–µ –∑–Ω–∞–π–¥–µ–Ω–∞. –í–∏–¥–∞–ª—è—î–º–æ state.json.")
            os.remove("state.json")

        jobs = response.css(DjinniSelectors.JOBS_LIST_SELECTOR)
        item: DjinniScraperItem = DjinniScraperItem()
        for job in jobs:
            title = job.css("h2 > a::text").get(default="Not specified").strip()
            salary = job.css("h2 > strong > span::text").get(default="Not specified").strip()
            company = job.css("a.text-body.js-analytics-event::text").get(default="Not specified").strip()
            views = job.css("div > div:nth-child(2) > span:nth-child(2)::text").get(default="Not specified").strip()
            responses = job.css("div > div:nth-child(2) > span:nth-child(4)::text").get(default="Not specified").strip()
            pub_date = job.css("div > div:nth-child(2) > span:nth-child(6)::attr(data-original-title)").get(default="Not specified").strip()
            truncate_description = job.css("span.js-truncated-text::text").get(default="Not specified").strip()
            raw_tags = job.css("h2 + div.fw-medium span::text").getall()
            tags = [tag.strip() for tag in raw_tags if tag.strip() and '¬∑' not in tag]
            url = response.urljoin(job.css("h2 > a::attr(href)").get(default="Not specified"))
            item.update(
                title=title,
                salary=salary,
                company=company,
                views=views,
                responses=responses,
                pub_date=pub_date,
                truncate_description=truncate_description,
                tags=tags,
                url=url,
                category=self.category,
            )
            yield item
        self.logger.info("üõë –ó–∞–∫—Ä–∏–≤–∞—î–º–æ —Å—Ç–æ—Ä—ñ–Ω–∫—É Playwright –ø–µ—Ä–µ–¥ –ø–µ—Ä–µ—Ö–æ–¥–æ–º –Ω–∞ –Ω–∞—Å—Ç—É–ø–Ω—É.")
        await page.close()
        async for next_page_request in self.parse_pagination(response):
            yield next_page_request

    async def parse_pagination(self, response):
        """–û–±—Ä–æ–±–ª—è—î –ø–∞–≥—ñ–Ω–∞—Ü—ñ—é —Ç–∞ –ø–µ—Ä–µ—Ö–æ–¥–∏—Ç—å –Ω–∞ –Ω–∞—Å—Ç—É–ø–Ω—É —Å—Ç–æ—Ä—ñ–Ω–∫—É."""
        next_page_element = response.css("li.page-item.active + li.page-item a.page-link")

        if not next_page_element:
            self.logger.info("üõë –ù–∞—Å—Ç—É–ø–Ω–æ—ó —Å—Ç–æ—Ä—ñ–Ω–∫–∏ –Ω–µ–º–∞—î. –ó–∞–≤–µ—Ä—à—É—î–º–æ –ø–∞—Ä—Å–∏–Ω–≥.")
            return

        next_page_value = next_page_element.css("::text").get(default="").strip()
        next_page_link = next_page_element.css("::attr(href)").get()

        if next_page_value.isdigit() and next_page_link:
            next_page_url = urllib_urljoin(response.url, next_page_link)
            self.logger.info(f"‚û° –ü–µ—Ä–µ—Ö–æ–¥–∏–º–æ –Ω–∞ —Å—Ç–æ—Ä—ñ–Ω–∫—É {next_page_value}: {next_page_url}")

            yield scrapy.Request(
                next_page_url,
                callback=self.parse_jobs,
                meta={
                    "playwright": True,
                    "playwright_context": "default",
                    "playwright_include_page": True,
                },
                dont_filter=True,  # ‚úÖ –î–æ–∑–≤–æ–ª—è—î Scrapy —Ä–æ–±–∏—Ç–∏ –ø–æ–≤—Ç–æ—Ä–Ω—ñ –∑–∞–ø–∏—Ç–∏
                errback=self.errback_close_page,
            )
        else:
            self.logger.warning("üö® `parse_pagination()` –Ω–µ –∑–Ω–∞–π—à–ª–∞ –Ω–∞—Å—Ç—É–ø–Ω–æ—ó —Å—Ç–æ—Ä—ñ–Ω–∫–∏!")

    async def errback_close_page(self, failure):
        """–ó–∞–∫—Ä–∏–≤–∞—î Playwright —Å—Ç–æ—Ä—ñ–Ω–∫—É —É –≤–∏–ø–∞–¥–∫—É –ø–æ–º–∏–ª–∫–∏."""
        page: Page = failure.request.meta.get("playwright_page")
        if page:
            await page.close()
            self.logger.warning("‚ùå –°—Ç–æ—Ä—ñ–Ω–∫–∞ Playwright –∑–∞–∫—Ä–∏—Ç–∞ —á–µ—Ä–µ–∑ –ø–æ–º–∏–ª–∫—É.")
        else:
            self.logger.error("üö® –ü–æ–º–∏–ª–∫–∞: –Ω–µ –≤–¥–∞–ª–æ—Å—è –æ—Ç—Ä–∏–º–∞—Ç–∏ `playwright_page` —É `errback`.")
